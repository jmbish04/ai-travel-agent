# AI Cascade Strategy - General Strategy

## General Cascade Principle

**Fast Methods â†’ LLM â†’ Fallback**

1. **Fast Methods** (NLP/Transformers/Heuristics) - for simple cases
2. **LLM** - for complex cases and contextual processing
3. **Fallback** - for cases when nothing worked

## Model Selection Strategy

### ğŸ”§ **Global Configuration**
```bash
# Single switch controls all NLP tasks
NLP_USE_LOCAL=true   # Use local models for all tasks
NLP_USE_LOCAL=false  # Use HF Inference API for all tasks
```

### ğŸ¯ **Task-Specific Models**

#### City/Location Recognition:
```
Local:  onnx/ner-bert-large-uncased-geocite (ONNX, optimized for cities)
Remote: Davlan/xlm-roberta-base-ner-hrl (HF Inference API)
```

#### General Entity Recognition:
```
Local:  Xenova/bert-base-multilingual-cased-ner-hrl
Remote: Davlan/xlm-roberta-base-ner-hrl (HF Inference API)
```

#### Intent Classification:
```
Local:  Xenova/nli-deberta-v3-base
Remote: facebook/bart-large-mnli (HF Inference API)
```

### ğŸš€ **Cascade Priority**
```
NLP_USE_LOCAL=true:  Local Models â†’ LLM â†’ Fallback
NLP_USE_LOCAL=false: Remote API â†’ LLM â†’ Fallback
```

### ğŸ“‹ Content Classification
```
Transformers â†’ LLM â†’ Rule-based Fallback
```
- **Transformers**: Fast classification by patterns
- **LLM**: Processing complex cases and context
- **Fallback**: Simple rules for obvious cases

### ğŸ¯ Intent Classification
```
Transformers â†’ LLM â†’ Keyword Fallback
```
- **Transformers**: Direct intent recognition
- **LLM**: Processing complex/composite queries
- **Fallback**: Keywords for basic intents

### ğŸ” Query Processing
```
Slot Memory â†’ LLM Router â†’ Pattern Matching
```
- **Slot Memory**: Using context from previous queries
- **LLM Router**: Complex routing considering history
- **Pattern Matching**: Simple rules for obvious cases

### ğŸŒ Search & Summarization
```
Query Optimization â†’ Search â†’ LLM Summarization â†’ Template Fallback
```
- **Query Optimization**: Improving query for search
- **Search**: Getting results
- **LLM Summarization**: Smart summarization with sources
- **Template Fallback**: Structured output without LLM

### âœ… Verification
```
Fact Checking â†’ LLM Audit â†’ Rule-based Validation
```
- **Fact Checking**: Checking facts against sources
- **LLM Audit**: Analyzing response quality
- **Rule-based**: Simple checks for compliance

## Cascade Selection Criteria

### When to use specific levels:

#### ğŸš€ Fast Methods (NLP/Transformers/Heuristics)
- âœ… High accuracy on simple cases
- âœ… Low latency
- âœ… Few resources (CPU/tokens)
- âœ… Deterministic result

#### ğŸ¤– LLM
- âœ… Complex logic/context
- âœ… Ambiguous cases
- âœ… Need user adaptation
- âœ… Need explanation/justification

#### ğŸ›¡ï¸ Fallback
- âœ… When LLM is unavailable
- âœ… For known patterns
- âœ… When deterministic result needed
- âœ… For critical cases

## Monitoring and Optimization

### Metrics to track:
- **Accuracy** by cascade levels
- **Response time** of each level
- **Usage frequency** of each level
- **Resources** (CPU, memory, tokens)

### Automatic optimization:
- Switching to LLM when fast method accuracy is low
- Caching results for repeating queries
- A/B testing different cascades

## Implementation Examples

### Entity Extraction for cities:
```
NER (LOC/GPE) â†’ Multi-word heuristics â†’ LLM disambiguation â†’ Regex patterns
```

### Content Classification:
```
Transformers zero-shot â†’ LLM context â†’ Keyword rules
```

### Intent Classification:
```
Transformers classification â†’ LLM complex cases â†’ Keyword matching
```